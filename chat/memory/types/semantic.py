from typing import List, Dict, Any, Optional, Set, Tuple
from datetime import datetime, timedelta
import json
import math
import numpy as np

from loguru import logger

from ..base import BaseMemory, MemoryItem, MemoryConfig
from ..embedding import get_dimension, get_text_embedder
from ...core.database_config import get_database_config

class Entity:
    """å®ä½“ç±»"""
    
    def __init__(
        self,
        entity_id: str,
        name: str,
        entity_type: str = "MISC",
        description: str = "",
        properties: Dict[str, Any] = None
    ):
        self.entity_id = entity_id
        self.name = name
        self.entity_type = entity_type  # PERSON, ORG, PRODUCT, SKILL, CONCEPTç­‰
        self.description = description
        self.properties = properties or {}
        self.created_at = datetime.now()
        self.updated_at = datetime.now()
        self.frequency = 1  # å‡ºç°é¢‘ç‡
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "entity_id": self.entity_id,
            "name": self.name,
            "entity_type": self.entity_type,
            "description": self.description,
            "properties": self.properties,
            "frequency": self.frequency
        }
    
class Relation:
    """å…³ç³»ç±»"""
    
    def __init__(
        self,
        from_entity: str,
        to_entity: str,
        relation_type: str,
        strength: float = 1.0,
        evidence: str = "",
        properties: Dict[str, Any] = None
    ):
        self.from_entity = from_entity
        self.to_entity = to_entity
        self.relation_type = relation_type
        self.strength = strength
        self.evidence = evidence  # æ”¯æŒè¯¥å…³ç³»çš„åŸæ–‡æœ¬
        self.properties = properties or {}
        self.created_at = datetime.now()
        self.frequency = 1  # å…³ç³»å‡ºç°é¢‘ç‡
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "from_entity": self.from_entity,
            "to_entity": self.to_entity,
            "relation_type": self.relation_type,
            "strength": self.strength,
            "evidence": self.evidence,
            "properties": self.properties,
            "frequency": self.frequency
        }

class SemanticMemory(BaseMemory):
    """è¯­ä¹‰è®°å¿†å®ç°"""

    def __init__(self, config: MemoryConfig, storage_backend=None):
        super().__init__(config, storage_backend)
        
        # åµŒå…¥æ¨¡å‹ï¼ˆç»Ÿä¸€æä¾›ï¼‰
        self.embedding_model = None
        self._init_embedding_model()

        # ä¸“ä¸šæ•°æ®åº“å­˜å‚¨
        self.vector_store = None
        self.graph_store = None
        self._init_databases()

        self.nlp = None
        self._init_nlp()

        logger.info("å¢å¼ºè¯­ä¹‰è®°å¿†åˆå§‹åŒ–å®Œæˆï¼ˆä½¿ç”¨Qdrant+Neo4jä¸“ä¸šæ•°æ®åº“ï¼‰")

    
    def _init_embedding_model(self):
        """åˆå§‹åŒ–ç»Ÿä¸€åµŒå…¥æ¨¡å‹ï¼ˆç”± embedding_provider ç®¡ç†ï¼‰ã€‚"""
        try:
            self.embedding_model = get_text_embedder()
            # è½»é‡å¥åº·æ£€æŸ¥ä¸æ—¥å¿—
            try:
                test_vec = self.embedding_model.encode("health_check")
                dim = getattr(self.embedding_model, "dimension", len(test_vec))
                logger.info(f"âœ… åµŒå…¥æ¨¡å‹å°±ç»ªï¼Œç»´åº¦: {dim}")
            except Exception:
                logger.info("âœ… åµŒå…¥æ¨¡å‹å°±ç»ª")
        except Exception as e:
            logger.error(f"âŒ åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    def _init_databases(self):
        """åˆå§‹åŒ–ä¸“ä¸šæ•°æ®åº“å­˜å‚¨"""
        try:
            # è·å–æ•°æ®åº“é…ç½®
            db_config = get_database_config()
            
            # åˆå§‹åŒ–Qdrantå‘é‡æ•°æ®åº“ï¼ˆä½¿ç”¨è¿æ¥ç®¡ç†å™¨é¿å…é‡å¤è¿æ¥ï¼‰
            from ..storage.qdrant_store import QdrantConnectionManager
            qdrant_config = db_config.get_qdrant_config() or {}
            qdrant_config["vector_size"] = get_dimension()
            self.vector_store = QdrantConnectionManager.get_instance(**qdrant_config)
            logger.info("âœ… Qdrantå‘é‡æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
            
            # åˆå§‹åŒ–Neo4jå›¾æ•°æ®åº“
            from ..storage.neo4j_store import Neo4jGraphStore
            neo4j_config = db_config.get_neo4j_config()
            self.graph_store = Neo4jGraphStore(**neo4j_config)
            logger.info("âœ… Neo4jå›¾æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
            
            # éªŒè¯è¿æ¥
            vector_health = self.vector_store.health_check()
            graph_health = self.graph_store.health_check()
            
            if not vector_health:
                logger.warning("âš ï¸ Qdrantè¿æ¥å¼‚å¸¸ï¼Œéƒ¨åˆ†åŠŸèƒ½å¯èƒ½å—é™")
            if not graph_health:
                logger.warning("âš ï¸ Neo4jè¿æ¥å¼‚å¸¸ï¼Œå›¾æœç´¢åŠŸèƒ½å¯èƒ½å—é™")
            
            logger.info(f"ğŸ¥ æ•°æ®åº“å¥åº·çŠ¶æ€: Qdrant={'âœ…' if vector_health else 'âŒ'}, Neo4j={'âœ…' if graph_health else 'âŒ'}")
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            logger.info("ğŸ’¡ è¯·æ£€æŸ¥æ•°æ®åº“é…ç½®å’Œç½‘ç»œè¿æ¥")
            logger.info("ğŸ’¡ å‚è€ƒ DATABASE_SETUP_GUIDE.md è¿›è¡Œé…ç½®")
            raise

    def _init_nlp(self):
        """åˆå§‹åŒ–NLPå¤„ç†å™¨ - æ™ºèƒ½å¤šè¯­è¨€æ”¯æŒ"""
        try:
            import spacy
            self.nlp_models = {}
            
            # å°è¯•åŠ è½½å¤šè¯­è¨€æ¨¡å‹
            models_to_try = [
                ("zh_core_web_sm", "ä¸­æ–‡"),
                ("en_core_web_sm", "è‹±æ–‡")
            ]
            
            loaded_models = []
            for model_name, lang_name in models_to_try:
                try:
                    nlp = spacy.load(model_name)
                    self.nlp_models[model_name] = nlp
                    loaded_models.append(lang_name)
                    logger.info(f"âœ… åŠ è½½{lang_name}spaCyæ¨¡å‹: {model_name}")
                except OSError:
                    logger.warning(f"âš ï¸ {lang_name}spaCyæ¨¡å‹ä¸å¯ç”¨: {model_name}")
            
            # è®¾ç½®ä¸»è¦NLPå¤„ç†å™¨
            if "zh_core_web_sm" in self.nlp_models:
                self.nlp = self.nlp_models["zh_core_web_sm"]
                logger.info("ğŸ¯ ä¸»è¦ä½¿ç”¨ä¸­æ–‡spaCyæ¨¡å‹")
            elif "en_core_web_sm" in self.nlp_models:
                self.nlp = self.nlp_models["en_core_web_sm"]
                logger.info("ğŸ¯ ä¸»è¦ä½¿ç”¨è‹±æ–‡spaCyæ¨¡å‹")
            else:
                self.nlp = None
                logger.warning("âš ï¸ æ— å¯ç”¨spaCyæ¨¡å‹ï¼Œå®ä½“æå–å°†å—é™")
            
            if loaded_models:
                logger.info(f"ğŸ“š å¯ç”¨è¯­è¨€æ¨¡å‹: {', '.join(loaded_models)}")
                
        except ImportError:
            logger.warning("âš ï¸ spaCyä¸å¯ç”¨ï¼Œå®ä½“æå–å°†å—é™")
            self.nlp = None
            self.nlp_models = {}

    def add(self, memory_item: MemoryItem) -> str:
        """æ·»åŠ è¯­ä¹‰è®°å¿†"""
        try:
            # 1. è®¡ç®—åµŒå…¥å‘é‡
            embedding = self.embedding_model.encode(memory_item.content)
            # å…¼å®¹ ndarray / list ç­‰å¤šç§è¿”å›ç±»å‹
            if hasattr(embedding, "tolist"):
                embedding = embedding.tolist()

            # 2. æå–å®ä½“å’Œå…³ç³»
            entities = self._extract_entities(memory_item.content)
            relations = self._extract_relations(memory_item.content, entities)

            # 3. å­˜å‚¨åˆ°Neo4jå›¾æ•°æ®åº“
            for entity in entities:
                self._add_entity_to_graph(entity, memory_item)
            
            for relation in relations:
                self._add_relation_to_graph(relation, memory_item)
            
            # 4. å­˜å‚¨åˆ°Qdrantå‘é‡æ•°æ®åº“
            metadata = {
                "memory_id": memory_item.id,
                "user_id": self.memory_type,
                "group_id": memory_item.group_id,
                "content": memory_item.content,
                "memory_type": memory_item.memory_type,
                "timestamp": int(memory_item.timestamp.timestamp()),
                "entities": [e.entity_id for e in entities],
                "entity_count": len(entities),
                "relation_count": len(relations)
            }

            success = self.vector_store.add_vector(
                vectors=[embedding],
                metadata=[metadata],
                ids=[memory_item.id]
            )

            if not success:
                logger.warning("âš ï¸ å‘é‡å­˜å‚¨å¤±è´¥ï¼Œä½†è®°å¿†å·²æ·»åŠ åˆ°å›¾æ•°æ®åº“")
            
            logger.info(f"âœ… æ·»åŠ è¯­ä¹‰è®°å¿†: {len(entities)}ä¸ªå®ä½“, {len(relations)}ä¸ªå…³ç³»")
            return memory_item.id
        
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ è¯­ä¹‰è®°å¿†å¤±è´¥: {e}")
            raise

    def _vector_search(self, query: str, top_k: int = 5, group_id: str = None) -> List[Dict[str, Any]]:
        """åœ¨å‘é‡æ•°æ®åº“ä¸­æœç´¢ç›¸å…³è®°å¿†"""
        try:
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_embedding = self.embedding_model.encode(query)
            if hasattr(query_embedding, "tolist"):
                query_embedding = query_embedding.tolist()
            
            # æ„å»ºè¿‡æ»¤æ¡ä»¶
            where_filter = {"memory_type": "semantic"}
            if group_id:
                where_filter["group_id"] = group_id

            # Qdrantå‘é‡æ£€ç´¢
            results = self.vector_store.search_vectors(
                query=query_embedding,
                top_k=top_k,
                where=where_filter if where_filter else None
            )

            # è½¬æ¢ç»“æœæ ¼å¼ï¼šä½¿ç”¨ payload.memory_id ä½œä¸ºé€»è¾‘IDï¼Œmetadata å•ç‹¬å­˜æ”¾
            formatted_results = []
            for result in results:
                meta = result.get("metadata", {}) or {}
                logical_id = meta.get("memory_id", result.get("id"))

                formatted_result = {
                    "id": logical_id,
                    "score": float(result.get("score", 0.0)),
                    "metadata": meta,
                    "content": meta.get("content", ""),
                    "user_id": meta.get("user_id"),
                    "group_id": meta.get("group_id"),
                    "memory_type": meta.get("memory_type"),
                    "timestamp": meta.get("timestamp"),
                    "entities": meta.get("entities", []),
                }
                formatted_results.append(formatted_result)

            logger.debug(f"ğŸ” Qdrantå‘é‡æœç´¢è¿”å› {len(formatted_results)} ä¸ªç»“æœ")
            return formatted_results
    
        except Exception as e:
            logger.error(f"âŒ å‘é‡æœç´¢å¤±è´¥: {e}")
            return []

    def _graph_search(self, query: str, limit: int, group_id: Optional[str] = None) -> List[Dict[str, Any]]:
        """Neo4jå›¾æœç´¢ï¼ˆç®€åŒ–ç‰ˆï¼šæŒ‰å®ä½“åç§°æŸ¥ç›¸å…³ memory_idï¼‰ã€‚

        å½“å‰å®ç°ä¸ä¾èµ– Entity.id çš„ hash ä¸€è‡´æ€§ï¼Œè€Œæ˜¯ï¼š
        1. ä»æŸ¥è¯¢ä¸­æŠ½å–å®ä½“ï¼ˆå¦‚ "åŒ—äº¬å¤§å­¦"ï¼‰ã€‚
        2. ç”¨åç§°æ¨¡ç³Šæœç´¢å›¾ä¸­çš„å®ä½“èŠ‚ç‚¹ã€‚
        3. ç›´æ¥ä»è¿™äº›å®ä½“èŠ‚ç‚¹çš„å±æ€§ä¸­è¯»å– memory_idã€‚
        """
        try:
            # ä»æŸ¥è¯¢ä¸­æå–å®ä½“
            query_entities = self._extract_entities(query)

            if not query_entities:
                # å¦‚æœæ²¡æœ‰æå–åˆ°å®ä½“ï¼Œå°è¯•æŒ‰åç§°æœç´¢
                entities_by_name = self.graph_store.search_entities_by_name(
                    name_pattern=query,
                    limit=10
                )
                if entities_by_name:
                    query_entities = [Entity(
                        entity_id=e["id"],
                        name=e["name"],
                        entity_type=e["type"]
                    ) for e in entities_by_name[:3]]
                else:
                    return []
            
            # ç›´æ¥é€šè¿‡å®ä½“åç§°ï¼Œä»å®ä½“èŠ‚ç‚¹å±æ€§ä¸­è¯»å– memory_id
            related_memory_ids: set[str] = set()

            for entity in query_entities:
                try:
                    entities_by_name = self.graph_store.search_entities_by_name(
                        name_pattern=entity.name,
                        limit=20,
                    )
                    for e in entities_by_name:
                        mem_id = e.get("memory_id")
                        if mem_id:
                            related_memory_ids.add(mem_id)
                except Exception as e:
                    logger.debug(f"å›¾æœç´¢å®ä½“ {entity.name} å¤±è´¥: {e}")
                    continue
            
            # æ„å»ºç»“æœ - ä»å‘é‡æ•°æ®åº“è·å–å®Œæ•´è®°å¿†ä¿¡æ¯_find_memory_by_id
            results = []
            for memory_id in list(related_memory_ids)[:limit * 2]:  # è·å–æ›´å¤šå€™é€‰
                try:
                    # ä¼˜å…ˆä»æœ¬åœ°ç¼“å­˜è·å–è®°å¿†è¯¦æƒ…ï¼Œé¿å…å ä½å‘é‡ç»´åº¦ä¸ä¸€è‡´é—®é¢˜
                    mem = self._find_memory_by_id(memory_id)
                    if not mem:
                        continue

                    if group_id and mem.group_id != group_id:
                        continue

                    metadata = {
                        "content": mem.content,
                        "user_id": mem.user_id,
                        "group_id": mem.group_id,
                        "memory_type": mem.memory_type,
                        "timestamp": int(mem.timestamp.timestamp()),
                        "entities": mem.metadata.get("entities", []),
                        "entity_count": mem.metadata.get("entity_count", 0),
                        "relation_count": mem.metadata.get("relation_count", 0),
                    }

                    # è®¡ç®—å›¾ç›¸å…³æ€§åˆ†æ•°
                    graph_score = self._calculate_graph_relevance_neo4j(metadata, query_entities)

                    results.append({
                        "id": memory_id,
                        "memory_id": memory_id,
                        "content": metadata.get("content", ""),
                        "similarity": graph_score,
                        "user_id": metadata.get("user_id"),
                        'group_id': metadata.get("group_id"),
                        "memory_type": metadata.get("memory_type"),
                        "timestamp": metadata.get("timestamp"),
                        "entities": metadata.get("entities", [])
                    })

                except Exception as e:
                    logger.debug(f"è·å–è®°å¿† {memory_id} è¯¦æƒ…å¤±è´¥: {e}")
                    continue
            
            # æŒ‰å›¾ç›¸å…³æ€§æ’åº
            results.sort(key=lambda x: x["similarity"], reverse=True)
            logger.debug(f"ğŸ•¸ï¸ Neo4jå›¾æœç´¢è¿”å› {len(results)} ä¸ªç»“æœ")
            return results[:limit]

        except Exception as e:
            logger.error(f"âŒ Neo4jå›¾æœç´¢å¤±è´¥: {e}")
            return []
        
    def _find_memory_by_id(self, memory_id: str) -> Optional[MemoryItem]:
        """æ ¹æ®è®°å¿†IDæŸ¥æ‰¾è®°å¿†é¡¹"""
        try:
            # é€šè¿‡ payload è¿‡æ»¤ memory_id è·å–ç‚¹æ•°æ®
            raw_results = self.vector_store.search_vectors(
                query=[0.0] * self.vector_store.vector_size,
                top_k=1,
                where={"memory_id": memory_id},
            )

            if not raw_results:
                return None

            data = raw_results[0].get("metadata", {})
            if not data:
                return None

            mem_item = MemoryItem(
                id=data.get("memory_id", ""),
                content=data.get("content", ""),
                memory_type=data.get("memory_type", ""),
                group_id=data.get("group_id", ""),
                user_id=data.get("user_id", ""),
                timestamp=datetime.fromtimestamp(data.get("timestamp", 0)),
                metadata=data,
            )
            return mem_item
        except Exception as e:
            logger.error(f"âŒ æŸ¥æ‰¾è®°å¿† {memory_id} å¤±è´¥: {e}")
            return None
        
    def _detect_language(self, text: str) -> str:
        """ç®€å•çš„è¯­è¨€æ£€æµ‹"""
        # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹ï¼ˆæ— æ­£åˆ™ï¼Œé€å­—ç¬¦åˆ¤æ–­èŒƒå›´ï¼‰
        chinese_chars = sum(1 for ch in text if '\u4e00' <= ch <= '\u9fff')
        total_chars = len(text.replace(' ', ''))
        
        if total_chars == 0:
            return "en"
        
        chinese_ratio = chinese_chars / total_chars
        return "zh" if chinese_ratio > 0.3 else "en"
    
    def _extract_entities(self, text: str) -> List[Entity]:
        """æ™ºèƒ½å¤šè¯­è¨€å®ä½“æå–"""
        entities = []
        
        # æ£€æµ‹æ–‡æœ¬è¯­è¨€
        lang = self._detect_language(text)
        
        # é€‰æ‹©åˆé€‚çš„spaCyæ¨¡å‹
        selected_nlp = None
        if lang == "zh" and "zh_core_web_sm" in self.nlp_models:
            selected_nlp = self.nlp_models["zh_core_web_sm"]
        elif lang == "en" and "en_core_web_sm" in self.nlp_models:
            selected_nlp = self.nlp_models["en_core_web_sm"]
        else:
            # ä½¿ç”¨é»˜è®¤æ¨¡å‹
            selected_nlp = self.nlp
        
        logger.debug(f"ğŸŒ æ£€æµ‹è¯­è¨€: {lang}, ä½¿ç”¨æ¨¡å‹: {selected_nlp.meta['name'] if selected_nlp else 'None'}")
        
        # ä½¿ç”¨spaCyè¿›è¡Œå®ä½“è¯†åˆ«å’Œè¯æ³•åˆ†æ
        if selected_nlp:
            try:
                doc = selected_nlp(text)
                logger.debug(f"ğŸ“ spaCyå¤„ç†æ–‡æœ¬: '{text}' -> {len(doc.ents)} ä¸ªå®ä½“")
                
                # å­˜å‚¨è¯æ³•åˆ†æç»“æœï¼Œä¾›Neo4jä½¿ç”¨
                self._store_linguistic_analysis(doc, text)
                
                if not doc.ents:
                    # å¦‚æœæ²¡æœ‰å®ä½“ï¼Œè®°å½•è¯¦ç»†çš„è¯å…ƒä¿¡æ¯
                    logger.debug("ğŸ” æœªæ‰¾åˆ°å®ä½“ï¼Œè¯å…ƒåˆ†æ:")
                    for token in doc[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªè¯å…ƒ
                        logger.debug(f"   '{token.text}' -> POS: {token.pos_}, TAG: {token.tag_}, ENT_IOB: {token.ent_iob_}")
                
                for ent in doc.ents:
                    entity = Entity(
                        entity_id=f"entity_{hash(ent.text)}",
                        name=ent.text,
                        entity_type=ent.label_,
                        description=f"ä»æ–‡æœ¬ä¸­è¯†åˆ«çš„{ent.label_}å®ä½“"
                    )
                    entities.append(entity)
                    # å®‰å…¨è·å–ç½®ä¿¡åº¦ä¿¡æ¯
                    confidence = "N/A"
                    try:
                        if hasattr(ent._, 'confidence'):
                            confidence = getattr(ent._, 'confidence', 'N/A')
                    except:
                        confidence = "N/A"
                    
                    logger.debug(f"ğŸ·ï¸ spaCyè¯†åˆ«å®ä½“: '{ent.text}' -> {ent.label_} (ç½®ä¿¡åº¦: {confidence})")
                
            except Exception as e:
                logger.warning(f"âš ï¸ spaCyå®ä½“è¯†åˆ«å¤±è´¥: {e}")
                import traceback
                logger.debug(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        else:
            logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨çš„spaCyæ¨¡å‹è¿›è¡Œå®ä½“è¯†åˆ«")
        
        return entities
    
    def _store_linguistic_analysis(self, doc, text: str):
        """å­˜å‚¨spaCyè¯æ³•åˆ†æç»“æœåˆ°Neo4j"""
        if not self.graph_store:
            return
            
        try:
            # ä¸ºæ¯ä¸ªè¯å…ƒåˆ›å»ºèŠ‚ç‚¹
            for token in doc:
                # è·³è¿‡æ ‡ç‚¹ç¬¦å·å’Œç©ºæ ¼
                if token.is_punct or token.is_space:
                    continue
                    
                token_id = f"token_{hash(token.text + token.pos_)}"
                
                # æ·»åŠ è¯å…ƒèŠ‚ç‚¹åˆ°Neo4j
                self.graph_store.add_entity(
                    entity_id=token_id,
                    name=token.text,
                    entity_type="TOKEN",
                    properties={
                        "pos": token.pos_,        # è¯æ€§ï¼ˆNOUN, VERBç­‰ï¼‰
                        "tag": token.tag_,        # ç»†ç²’åº¦æ ‡ç­¾
                        "lemma": token.lemma_,    # è¯å…ƒåŸå½¢
                        "is_alpha": token.is_alpha,
                        "is_stop": token.is_stop,
                        "source_text": text[:50],  # æ¥æºæ–‡æœ¬ç‰‡æ®µ
                        "language": self._detect_language(text)
                    }
                )
                
                # å¦‚æœæ˜¯åè¯ï¼Œå¯èƒ½æ˜¯æ½œåœ¨çš„æ¦‚å¿µ
                if token.pos_ in ["NOUN", "PROPN"]:
                    concept_id = f"concept_{hash(token.text)}"
                    self.graph_store.add_entity(
                        entity_id=concept_id,
                        name=token.text,
                        entity_type="CONCEPT",
                        properties={
                            "category": token.pos_,
                            "frequency": 1,  # å¯ä»¥åç»­ç´¯è®¡
                            "source_text": text[:50]
                        }
                    )
                    
                    # å»ºç«‹è¯å…ƒåˆ°æ¦‚å¿µçš„å…³ç³»
                    self.graph_store.add_relationship(
                        from_entity_id=token_id,
                        to_entity_id=concept_id,
                        relationship_type="REPRESENTS",
                        properties={"confidence": 1.0}
                    )
            
            # å»ºç«‹è¯å…ƒä¹‹é—´çš„ä¾å­˜å…³ç³»
            for token in doc:
                if token.is_punct or token.is_space or token.head == token:
                    continue
                    
                from_id = f"token_{hash(token.text + token.pos_)}"
                to_id = f"token_{hash(token.head.text + token.head.pos_)}"
                
                # Neo4jä¸å…è®¸å…³ç³»ç±»å‹åŒ…å«å†’å·ï¼Œéœ€è¦æ¸…ç†
                relation_type = token.dep_.upper().replace(":", "_")
                
                self.graph_store.add_relationship(
                    from_entity_id=from_id,
                    to_entity_id=to_id,
                    relationship_type=relation_type,  # æ¸…ç†åçš„ä¾å­˜å…³ç³»ç±»å‹
                    properties={
                        "dependency": token.dep_,  # ä¿ç•™åŸå§‹ä¾å­˜å…³ç³»
                        "source_text": text[:50]
                    }
                )
            
            logger.debug(f"ğŸ”— å·²å°†è¯æ³•åˆ†æç»“æœå­˜å‚¨åˆ°Neo4j: {len([t for t in doc if not t.is_punct and not t.is_space])} ä¸ªè¯å…ƒ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ å­˜å‚¨è¯æ³•åˆ†æå¤±è´¥: {e}")

    def _extract_relations(self, text: str, entities: List[Entity]) -> List[Relation]:
        """æå–å…³ç³»"""
        relations = []
        # ä»…ä¿ç•™ç®€å•å…±ç°å…³ç³»ï¼Œä¸åšä»»ä½•æ­£åˆ™/å…³é”®è¯åŒ¹é…
        for i, entity1 in enumerate(entities):
            for entity2 in entities[i+1:]:
                relations.append(Relation(
                    from_entity=entity1.entity_id,
                    to_entity=entity2.entity_id,
                    relation_type="CO_OCCURS",
                    strength=0.5,
                    evidence=text[:100]
                ))
        return relations
    
    def _add_entity_to_graph(self, entity: Entity, memory_item: MemoryItem):
        """æ·»åŠ å®ä½“åˆ°Neo4jå›¾æ•°æ®åº“"""
        try:
            # å‡†å¤‡å®ä½“å±æ€§
            properties = {
                "name": entity.name,
                "description": entity.description,
                "frequency": entity.frequency,
                "memory_id": memory_item.id,
                "user_id": memory_item.user_id,
                "group_id": memory_item.group_id,
                **entity.properties
            }
            
            # æ·»åŠ åˆ°Neo4j
            success = self.graph_store.add_entity(
                entity_id=entity.entity_id,
                name=entity.name,
                entity_type=entity.entity_type,
                properties=properties
            )
                    
            return success
            
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ å®ä½“åˆ°å›¾æ•°æ®åº“å¤±è´¥: {e}")
            return False
        
    def _add_relation_to_graph(self, relation: Relation, memory_item: MemoryItem):
        """æ·»åŠ å…³ç³»åˆ°Neo4jå›¾æ•°æ®åº“"""
        try:
            # å‡†å¤‡å…³ç³»å±æ€§
            properties = {
                "strength": relation.strength,
                "memory_id": memory_item.id,
                "user_id": memory_item.user_id,
                "group_id": memory_item.group_id,
                "evidence": relation.evidence
            }
            
            # æ·»åŠ åˆ°Neo4j
            success = self.graph_store.add_relationship(
                from_entity_id=relation.from_entity,
                to_entity_id=relation.to_entity,
                relationship_type=relation.relation_type,
                properties=properties
            )
                
            return success
            
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ å…³ç³»åˆ°å›¾æ•°æ®åº“å¤±è´¥: {e}")
            return False
        
    def _calculate_graph_relevance_neo4j(self, memory_metadata: Dict[str, Any], query_entities: List[Entity]) -> float:
        """è®¡ç®—Neo4jå›¾ç›¸å…³æ€§åˆ†æ•°"""
        try:
            memory_entities = memory_metadata.get("entities", [])
            if not memory_entities or not query_entities:
                return 0.0
            
            # å®ä½“åŒ¹é…åº¦
            query_entity_ids = {e.entity_id for e in query_entities}
            matching_entities = len(set(memory_entities).intersection(query_entity_ids))
            entity_score = matching_entities / len(query_entity_ids) if query_entity_ids else 0
            
            # å®ä½“æ•°é‡åŠ æƒ
            entity_count = memory_metadata.get("entity_count", 0)
            entity_density = min(entity_count / 10, 1.0)  # å½’ä¸€åŒ–åˆ°[0,1]
            
            # å…³ç³»æ•°é‡åŠ æƒ
            relation_count = memory_metadata.get("relation_count", 0)
            relation_density = min(relation_count / 5, 1.0)  # å½’ä¸€åŒ–åˆ°[0,1]
            
            # ç»¼åˆåˆ†æ•°
            relevance_score = (
                entity_score * 0.6 +           # å®ä½“åŒ¹é…æƒé‡60%
                entity_density * 0.2 +         # å®ä½“å¯†åº¦æƒé‡20%
                relation_density * 0.2         # å…³ç³»å¯†åº¦æƒé‡20%
            )
            
            return min(relevance_score, 1.0)
            
        except Exception as e:
            logger.debug(f"è®¡ç®—å›¾ç›¸å…³æ€§å¤±è´¥: {e}")
            return 0.0
    
    def retrieve(self, query: str, top_k: int = 5, **kwargs) -> List[MemoryItem]:
        """æ£€ç´¢è¯­ä¹‰è®°å¿†"""
        try:
            group_id = kwargs.get("group_id")

            # 1. å‘é‡æœç´¢
            vector_results = self._vector_search(query, top_k=top_k, group_id=group_id)
            
            # 2. å›¾æœç´¢
            graph_results = self._graph_search(query, limit=top_k, group_id=group_id)

            # 3. æ··åˆæ’åº
            combined_results = self._combine_and_rank_results(
                vector_results, graph_results, query, top_k
            )

            # 3.1 å¯¹ combined_score è¿›è¡Œå½’ä¸€åŒ–
            scores = [r.get("combined_score", r.get("vector_score", 0.0)) for r in combined_results]
            if scores:
                import math
                max_s = max(scores)
                exps = [math.exp(s - max_s) for s in scores]
                denom = sum(exps) or 1.0
                probs = [e / denom for e in exps]
            else:
                probs = []

            # 4. è¿‡æ»¤å·²é—å¿˜è®°å¿†å¹¶è½¬æ¢ä¸ºMemoryItem
            result_memories = []
            for idx, result in enumerate(combined_results):
                # å¤„ç†æ—¶é—´æˆ³
                timestamp = result.get("timestamp")
                if isinstance(timestamp, str):
                    try:
                        timestamp = datetime.fromisoformat(timestamp)
                    except ValueError:
                        timestamp = datetime.now()
                elif isinstance(timestamp, (int, float)):
                    timestamp = datetime.fromtimestamp(timestamp)
                else:
                    timestamp = datetime.now()
                
                # ç›´æ¥ä»ç»“æœæ•°æ®æ„å»ºMemoryItemï¼ˆé™„å¸¦åˆ†æ•°ä¸æ¦‚ç‡ï¼‰
                memory_item = MemoryItem(
                    id=result["id"],
                    content=result["content"],
                    memory_type="semantic",
                    user_id=result.get("user_id", "default"),
                    group_id=result.get("group_id", None),
                    timestamp=timestamp,
                    metadata={
                        **result.get("metadata", {}),
                        "combined_score": result.get("combined_score", 0.0),
                        "vector_score": result.get("vector_score", 0.0),
                        "graph_score": result.get("graph_score", 0.0),
                        "probability": probs[idx] if idx < len(probs) else 0.0,
                    }
                )
                result_memories.append(memory_item)
            
            logger.info(f"âœ… æ£€ç´¢åˆ° {len(result_memories)} æ¡ç›¸å…³è®°å¿†")
            return result_memories[:top_k]
                
        except Exception as e:
            logger.error(f"âŒ æ£€ç´¢è¯­ä¹‰è®°å¿†å¤±è´¥: {e}")
            return []
            

    def _combine_and_rank_results(
        self,
        vector_results: List[Dict[str, Any]],
        graph_results: List[Dict[str, Any]],
        query: str,
        top_k: int
    ) -> List[Dict[str, Any]]:
        """ç»“åˆå‘é‡å’Œå›¾æœç´¢ç»“æœå¹¶é‡æ–°æ’åº"""
        # åˆå¹¶ç»“æœï¼ŒæŒ‰å†…å®¹å»é‡
        combined = {}
        content_seen = set()

        # æ·»åŠ å‘é‡ç»“æœ
        for result in vector_results:
            memory_id = result["id"]
            content = result.get("content", "")

            # å†…å®¹å»é‡
            content_hash = hash(content.strip())
            if content_hash in content_seen:
                logger.debug(f"è·³è¿‡é‡å¤å†…å®¹çš„å‘é‡ç»“æœ: {content[:30]}...")
                continue

            content_seen.add(content_hash)
            combined[memory_id] = {
                **result,
                "content_hash": content_hash,
                "vector_score": result.get("score", 0.0),
                "graph_score": 0.0  # é»˜è®¤å›¾åˆ†æ•°ä¸º0
            }

        # æ·»åŠ å›¾ç»“æœ
        for result in graph_results:
            memory_id = result["id"]
            content = result.get("content", "")
            content_hash = hash(content.strip())

            if memory_id in combined:
                # å·²å­˜åœ¨ï¼Œæ›´æ–°å›¾åˆ†æ•°
                combined[memory_id]["graph_score"] = result.get("similarity", 0.0)
            elif content_hash not in content_seen:
                content_seen.add(content_hash)
                combined[memory_id] = {
                    **result,
                    "content_hash": content_hash,
                    "vector_score": 0.0,  # é»˜è®¤å‘é‡åˆ†æ•°ä¸º0
                    "graph_score": result.get("similarity", 0.0)
                }

        # è®¡ç®—æ··åˆåˆ†æ•°
        for memory_id, result in combined.items():
            vector_score = result.get("vector_score", 0.0)
            graph_score = result.get("graph_score", 0.0)

            # ç®€å•åŠ æƒå¹³å‡
            mixed_score = (vector_score * 0.7) + (graph_score * 0.3)
            
            result["debug_info"] = {
                "vector_score": vector_score,
                "graph_score": graph_score,
                "mixed_score": mixed_score
            }
            
            result["combined_score"] = mixed_score

        # åº”ç”¨æœ€å°ç›¸å…³æ€§é˜ˆå€¼
        min_threshold = 0.1  # æœ€å°ç›¸å…³æ€§é˜ˆå€¼
        filtered_results = [
            result for result in combined.values() 
            if result["combined_score"] >= min_threshold
        ]

        # æ’åºå¹¶è¿”å›
        sorted_results = sorted(
            filtered_results,
            key=lambda x: x["combined_score"],
            reverse=True
        )

        # è°ƒè¯•ä¿¡æ¯
        logger.debug(f"ğŸ” å‘é‡ç»“æœ: {len(vector_results)}, å›¾ç»“æœ: {len(graph_results)}")
        logger.debug(f"ğŸ“ å»é‡å: {len(combined)}, è¿‡æ»¤å: {len(filtered_results)}")

        for i, result in enumerate(sorted_results[:3]):
            logger.debug(f"  ç»“æœ{i+1}: å‘é‡={result['vector_score']:.3f}, å›¾={result['graph_score']:.3f}, ç²¾ç¡®={result.get('exact_match_bonus', 0):.3f}, å…³é”®è¯={result.get('keyword_bonus', 0):.3f}, å…¬å¸={result.get('company_bonus', 0):.3f}, å®ä½“={result.get('entity_type_bonus', 0):.3f}, ç»¼åˆ={result['combined_score']:.3f}")
        
        return sorted_results[:top_k]

    def update(
        self,
        memory_id: str,
        content: str = None,
        metadata: Dict[str, Any] = None
    ) -> bool:
        """æ›´æ–°è¯­ä¹‰è®°å¿†ï¼ˆç®€å•ç­–ç•¥ï¼šåˆ é™¤åé‡å»ºï¼‰"""
        try:
            # 1. å…ˆæŸ¥å‡ºæ—§è®°å¿†
            old = self._find_memory_by_id(memory_id)
            if not old:
                logger.warning(f"âš ï¸ å¾…æ›´æ–°è®°å¿†ä¸å­˜åœ¨: {memory_id}")
                return False

            # 2. åˆ é™¤æ—§å‘é‡ï¼ˆå›¾æ•°æ®ç”±é‡æ–° add æ—¶è¦†ç›–/è¿½åŠ ï¼‰
            try:
                # ä¾èµ– QdrantVectorStore.delete_memories æŒ‰ payload.memory_id åˆ é™¤
                if hasattr(self.vector_store, "delete_memories"):
                    self.vector_store.delete_memories([memory_id])
            except Exception as e:
                logger.warning(f"âš ï¸ åˆ é™¤æ—§å‘é‡å¤±è´¥ï¼Œç»§ç»­å°è¯•é‡å»º: {e}")

            # 3. ç»„è£…æ–°çš„ MemoryItem
            new_content = content if content is not None else old.content
            new_metadata = dict(old.metadata or {})
            if metadata:
                new_metadata.update(metadata)

            new_item = MemoryItem(
                id=old.id,
                content=new_content,
                memory_type=old.memory_type,
                group_id=old.group_id,
                user_id=old.user_id,
                timestamp=datetime.now(),
                metadata=new_metadata,
            )

            # 4. é‡æ–°å†™å…¥ï¼ˆå‘é‡ + å®ä½“/å…³ç³»ï¼‰
            self.add(new_item)
            logger.info(f"âœ… æ›´æ–°è¯­ä¹‰è®°å¿†å®Œæˆ: {memory_id}")
            return True

        except Exception as e:
            logger.error(f"âŒ æ›´æ–°è¯­ä¹‰è®°å¿†å¤±è´¥ {memory_id}: {e}")
            return False
    
    def remove(self, memory_id: str) -> bool:
        """åˆ é™¤è¯­ä¹‰è®°å¿†ï¼šåŒæ—¶ä»å‘é‡åº“(Qdrant)å’Œå›¾æ•°æ®åº“(Neo4j)ä¸­åˆ é™¤ã€‚

        Qdrant ä¾§æŒ‰ payload.memory_id åˆ é™¤ï¼›Neo4j ä¾§åˆ é™¤ä¸è¯¥è®°å¿†ç›¸å…³çš„å®ä½“/å…³ç³»ã€‚
        """
        success = True

        # 1ï¼‰åˆ é™¤ Qdrant ä¸­å¯¹åº”å‘é‡
        try:
            if self.vector_store is not None:
                # ä¼˜å…ˆä½¿ç”¨æŒ‰ memory_id åˆ é™¤çš„æ¥å£ï¼Œä¿æŒä¸ update/forget è¯­ä¹‰ä¸€è‡´
                if hasattr(self.vector_store, "delete_memories"):
                    self.vector_store.delete_memories([memory_id])
                elif hasattr(self.vector_store, "delete_vector"):
                    # å‘åå…¼å®¹æ—§çš„æŒ‰ point-id åˆ é™¤æ¥å£
                    self.vector_store.delete_vector([memory_id])
        except Exception as e:
            logger.warning(f"âš ï¸ åˆ é™¤Qdrantå‘é‡å¤±è´¥: {e}")
            success = False

        # 2ï¼‰åˆ é™¤ Neo4j ä¸­ä¸è¯¥è®°å¿†ç›¸å…³çš„å®ä½“/å…³ç³»
        try:
            if self.graph_store is not None:
                # åˆ é™¤æ‰€æœ‰ memory_id åŒ¹é…çš„å®ä½“èŠ‚ç‚¹ï¼ˆå«å…¶å…³ç³»ï¼‰
                query = """
                MATCH (e:Entity {memory_id: $memory_id})
                DETACH DELETE e
                """
                session = getattr(self.graph_store, "driver", None)
                database = getattr(self.graph_store, "database", "neo4j")
                if session is not None:
                    with session.session(database=database) as neo_session:
                        neo_session.run(query, memory_id=memory_id)
        except Exception as e:
            logger.warning(f"âš ï¸ åˆ é™¤Neo4jå®ä½“å¤±è´¥: {e}")
            success = False

        if success:
            logger.info(f"âœ… å·²åˆ é™¤è¯­ä¹‰è®°å¿†: {memory_id}")
        else:
            logger.warning(f"âš ï¸ è¯­ä¹‰è®°å¿†åˆ é™¤éƒ¨åˆ†å¤±è´¥: {memory_id}")

        return success
        
    def has_memory(self, memory_id: str) -> bool:
        """æ£€æŸ¥è®°å¿†æ˜¯å¦å­˜åœ¨"""
        return self._find_memory_by_id(memory_id) is not None
    
    def clear(self):
        """æ¸…ç©ºæ‰€æœ‰è¯­ä¹‰è®°å¿† - åŒ…æ‹¬ä¸“ä¸šæ•°æ®åº“"""
        try:
            # æ¸…ç©ºQdrantå‘é‡æ•°æ®åº“
            if self.vector_store:
                success = self.vector_store.clear_collection()
                if success:
                    logger.info("âœ… Qdrantå‘é‡æ•°æ®åº“å·²æ¸…ç©º")
                else:
                    logger.warning("âš ï¸ Qdrantæ¸…ç©ºå¤±è´¥")
            
            # æ¸…ç©ºNeo4jå›¾æ•°æ®åº“
            if self.graph_store:
                success = self.graph_store.clear_all()
                if success:
                    logger.info("âœ… Neo4jå›¾æ•°æ®åº“å·²æ¸…ç©º")
                else:
                    logger.warning("âš ï¸ Neo4jæ¸…ç©ºå¤±è´¥")
            
            logger.info("ğŸ§¹ è¯­ä¹‰è®°å¿†ç³»ç»Ÿå·²å®Œå…¨æ¸…ç©º")
            
        except Exception as e:
            logger.error(f"âŒ æ¸…ç©ºè¯­ä¹‰è®°å¿†å¤±è´¥: {e}")
    
    def get_all(self) -> List[MemoryItem]:
        """è·å–æ‰€æœ‰è¯­ä¹‰è®°å¿†"""
        # æš‚æ—¶ä¸å®ç°
        return []

    def get_stats(self) -> Dict[str, Any]:
        """è·å–è¯­ä¹‰è®°å¿†ç»Ÿè®¡ä¿¡æ¯"""
        return {}

    def export_knowledge_graph(self) -> Dict[str, Any]:
        """å¯¼å‡ºçŸ¥è¯†å›¾è°± - ä»Neo4jè·å–ç»Ÿè®¡ä¿¡æ¯"""
        try:
            # ä»Neo4jè·å–ç»Ÿè®¡ä¿¡æ¯
            stats = {}
            if self.graph_store:
                stats = self.graph_store.get_stats()
            
            return {
                "graph_stats": {
                    "total_nodes": stats.get("total_nodes", 0),
                    "entity_nodes": stats.get("entity_nodes", 0),
                    "memory_nodes": stats.get("memory_nodes", 0),
                    "total_relationships": stats.get("total_relationships", 0),
                }
            }
        except Exception as e:
            logger.error(f"âŒ å¯¼å‡ºçŸ¥è¯†å›¾è°±å¤±è´¥: {e}")
            return {
                "entities": {},
                "relations": [],
                "graph_stats": {"error": str(e)}
            }